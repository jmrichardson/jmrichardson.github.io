{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Financial Machine Learning framework is a modular library for automating financial machine learning workflows, integration of algorithmic strategies, and a platform for sharing reusable code. Designed with re-usable components, FML abstracts away the complexity of creating algorithmic and machine learning workflows with minimal lines of python code. Modular and reusable components such as feature engineering, transformations, dimension reduction, models, algorigmithic strategies, etc can easily be leveraged. FML also enables powerful stacking ensemble techniques across multiple data splits. Overview Quantitative trading analysts often reuse code in numerous strategies and also frequently build similar workflows. FML seeks to leverage existing and reusable code in a modular and structured fashion. The code can easily be leveraged using a standard sci-kit interface and continually improved. In addition, quant workflows generally consist of repeating processes in strategy development. FML implements flexible pipelines to rapidly build flexible and complex workflows. Professional and proprietary frameworks exist to streamline the effort, but few are open, python based and provide a flexible and structured framework to streamline the strategy lifecycle. FML can be used to leverage pre-built modules in your own workflow. This enables the reuse of code in the popular sci-kit fit/transform interface. In addition, pipelines can be configured to rapidly create customized workflows. Pipelines are easily configured by: Determine the data splits to model. Split methodologies such as rolling window (both anchored and expanding), regime detection or simple train/test split are used to define what data partitions are fed into each model. Test data (out of sample) is also defined to measure the model(s) efficacy. Choose stack architecture. The predictions of earlier models can easily be used as inputs into later models. This stacking technique enables an ensemble approach to model predictions. FML handles the plumbing necessary for many models across multiple data splits. Define one or many workflow(s). Simply select from the available modules for targets, features, transforms, feature reduction and models. Project Goals Provide a flexible, automated and easy-to-use implementation of machine learning workflows for systematic trading. The main tenants of a workflow consists of features, transformations, reduction, models and targets applied against a data split definition for multiple data frequencies. Models can be stacked across data splits in many ways, and multiple workflows can be utilized to define simple to complex configurations. Provide a comprehensive library of algorithmic and machine learning modules. All modules are sci-kit compatible with familiar fit/transform methods enabling consistent and uniform interfaces. Combine machine learning with quantitative trading strategies in an easy to use framework.","title":"Home"},{"location":"#overview","text":"Quantitative trading analysts often reuse code in numerous strategies and also frequently build similar workflows. FML seeks to leverage existing and reusable code in a modular and structured fashion. The code can easily be leveraged using a standard sci-kit interface and continually improved. In addition, quant workflows generally consist of repeating processes in strategy development. FML implements flexible pipelines to rapidly build flexible and complex workflows. Professional and proprietary frameworks exist to streamline the effort, but few are open, python based and provide a flexible and structured framework to streamline the strategy lifecycle. FML can be used to leverage pre-built modules in your own workflow. This enables the reuse of code in the popular sci-kit fit/transform interface. In addition, pipelines can be configured to rapidly create customized workflows. Pipelines are easily configured by: Determine the data splits to model. Split methodologies such as rolling window (both anchored and expanding), regime detection or simple train/test split are used to define what data partitions are fed into each model. Test data (out of sample) is also defined to measure the model(s) efficacy. Choose stack architecture. The predictions of earlier models can easily be used as inputs into later models. This stacking technique enables an ensemble approach to model predictions. FML handles the plumbing necessary for many models across multiple data splits. Define one or many workflow(s). Simply select from the available modules for targets, features, transforms, feature reduction and models.","title":"Overview"},{"location":"#project-goals","text":"Provide a flexible, automated and easy-to-use implementation of machine learning workflows for systematic trading. The main tenants of a workflow consists of features, transformations, reduction, models and targets applied against a data split definition for multiple data frequencies. Models can be stacked across data splits in many ways, and multiple workflows can be utilized to define simple to complex configurations. Provide a comprehensive library of algorithmic and machine learning modules. All modules are sci-kit compatible with familiar fit/transform methods enabling consistent and uniform interfaces. Combine machine learning with quantitative trading strategies in an easy to use framework.","title":"Project Goals"},{"location":"data/","text":"Importing Price Data AIM supports any frequency data (only a handful have been tested so far). In addition, different time frames can be used within the same pipeline. For example, feeding the predictions of a weekly model to a daily model (see feed architecture ). Data Preparation A folder named prices must exist within the current working directory containing a frequency sub-folder with ticker CSV files in the following sample hiearchy: prices/ --weekly/ --SPY.csv --GLD.csv --daily/ --SPY.csv --GLD.csv --minute15/ --SPY.csv --GLD.csv Optionally, a markets sub-folder containing groups of tickers: prices/ --markets/ --etf_sectors.csv Prices To load price data for a market of equities inside a pipeline, provide a list of tickers and frequency(s) for which the raw prices exist in the prices sub-folder. prices=aim.Prices(tickers=tickers, frequency=['daily'], start_date='2017-01-01', end_date='2020-12-31', look_back=200), tickers: List of tickers eg: ['SPY', 'GLD'] frequency: Name of prices/{frequency} sub-folder start_date: Start date of prices to import end_date: End data of prices to import look_back: Number of periods to include for feature calculation. Specifically, data prior to start date will be imported as well as for each split data partition. Returns a list of market prices for each frequency included See examples for more information","title":"Data"},{"location":"data/#importing-price-data","text":"AIM supports any frequency data (only a handful have been tested so far). In addition, different time frames can be used within the same pipeline. For example, feeding the predictions of a weekly model to a daily model (see feed architecture ).","title":"Importing Price Data"},{"location":"data/#data-preparation","text":"A folder named prices must exist within the current working directory containing a frequency sub-folder with ticker CSV files in the following sample hiearchy: prices/ --weekly/ --SPY.csv --GLD.csv --daily/ --SPY.csv --GLD.csv --minute15/ --SPY.csv --GLD.csv Optionally, a markets sub-folder containing groups of tickers: prices/ --markets/ --etf_sectors.csv","title":"Data Preparation"},{"location":"data/#prices","text":"To load price data for a market of equities inside a pipeline, provide a list of tickers and frequency(s) for which the raw prices exist in the prices sub-folder. prices=aim.Prices(tickers=tickers, frequency=['daily'], start_date='2017-01-01', end_date='2020-12-31', look_back=200), tickers: List of tickers eg: ['SPY', 'GLD'] frequency: Name of prices/{frequency} sub-folder start_date: Start date of prices to import end_date: End data of prices to import look_back: Number of periods to include for feature calculation. Specifically, data prior to start date will be imported as well as for each split data partition. Returns a list of market prices for each frequency included See examples for more information","title":"Prices"},{"location":"install/","text":"The best choice to install FML is to clone the repo and pip install -e from source. This enables active development from local source code while still using the package as if it were installed: git clone https://github.com/jmrichardson/fml.git cd fml pip install -e . Note: FML is a private repo, therefore you will need to clone using your authentication token","title":"Install"},{"location":"issues/","text":"xgboost does not support -1, 1 binary classification. Not sure if it is wise to force -1, 1 to 0, 1 Pycaret multiclass (-1, 0, 1) scoring issue. Not all models have this issue but confirmed lightgbm and nb do (others likely). It seems to happen when a split doesn't have enough data (rows or columns). This is apparently fixed in pycaret 3.0: ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].","title":"Known Issues"},{"location":"todo/","text":"Add reporting using datapane. Each module should also be able to report Update backtrader module. Currently only has limited functionality Update documentation Debug all workflows and modules Upgrade to pycaret when available","title":"To Do"},{"location":"modules/components/","text":"Available Component Modules Data Prices Splits Rolling window (walk forward) Rolling expanding window (walk forward) Dynamic Regime Detection Train Test Split Feed Architecture None Forward Forward Combine Last Last Combine Pre-process To be added Feature Engineering TuneTA Formulaic alphas 101 Catch 22 Qlib Lagged returns Date time features Transformations Remove constant features Fractional difference stationary features Feature selection (BoostBoruta) Models Pycaret Targets Direction Forward Return Triple Barrier Label Trend Scanning Tail Sets Events Cusum Signals Moving Average Crossover","title":"Components"},{"location":"modules/components/#available-component-modules","text":"","title":"Available Component Modules"},{"location":"modules/components/#data","text":"Prices","title":"Data"},{"location":"modules/components/#splits","text":"Rolling window (walk forward) Rolling expanding window (walk forward) Dynamic Regime Detection Train Test Split","title":"Splits"},{"location":"modules/components/#feed-architecture","text":"None Forward Forward Combine Last Last Combine","title":"Feed Architecture"},{"location":"modules/components/#pre-process","text":"To be added","title":"Pre-process"},{"location":"modules/components/#feature-engineering","text":"TuneTA Formulaic alphas 101 Catch 22 Qlib Lagged returns Date time features","title":"Feature Engineering"},{"location":"modules/components/#transformations","text":"Remove constant features Fractional difference stationary features Feature selection (BoostBoruta)","title":"Transformations"},{"location":"modules/components/#models","text":"Pycaret","title":"Models"},{"location":"modules/components/#targets","text":"Direction Forward Return Triple Barrier Label Trend Scanning Tail Sets","title":"Targets"},{"location":"modules/components/#events","text":"Cusum","title":"Events"},{"location":"modules/components/#signals","text":"Moving Average Crossover","title":"Signals"},{"location":"modules/targets/direction/","text":"xgboost does not support -1, 1 binary classification. Not sure if it is wise to force -1, 1 to 0, 1 lightgbm multiclass (-1, 0, 1) scoring issue. This is fixed in pycaret 3.0: ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].","title":"Direction"},{"location":"modules/targets/targets/","text":"Add reporting using datapane. Each module should also be able to report Update backtrader module. Currently only has limited functionality Update documentation Debug all workflows and modules Upgrade to pycaret when available","title":"Targets"},{"location":"pipeline/pipeline/","text":"Feed Architecture It's easy to create independent models within a pipeline. It's also easy to stack models so that predictions from earlier models can be used as features to later models by simply defining the feed type. This makes the following possible: Ensemble/stacking approach Model(s) learn from all splits as well as other model(s) feature space Feed predictions from models of multiple time frames to later models Feature space reduction partitioned by model and integrated in successive models All model predictions regardless of split are sent to final Predictions module where simple weighting or other aggregation methodology can be leveraged None Output all model predictions for each split partition. feed=None Forward Output all model predictions for each split partition. In addition, the predictions of earlier models are sent to later models of the same workflow. feed='forward' Forward Combine Output all model predictions for each split partition. In addition, the predictions of earlier models are sent to later models of all workflows. feed='forward_combine' Last Output all model predictions for each split partition. In addition, the predictions of all earlier models are sent to the last model of the same workflow. feed='last' Last Combine Output all model predictions for each split partition. In addition, the predictions of all earlier models are sent to the last model of all workflows. feed='last_combine' Examples: See examples for more information","title":"Pipeline"},{"location":"pipeline/pipeline/#feed-architecture","text":"It's easy to create independent models within a pipeline. It's also easy to stack models so that predictions from earlier models can be used as features to later models by simply defining the feed type. This makes the following possible: Ensemble/stacking approach Model(s) learn from all splits as well as other model(s) feature space Feed predictions from models of multiple time frames to later models Feature space reduction partitioned by model and integrated in successive models All model predictions regardless of split are sent to final Predictions module where simple weighting or other aggregation methodology can be leveraged","title":"Feed Architecture"},{"location":"pipeline/pipeline/#none","text":"Output all model predictions for each split partition. feed=None","title":"None"},{"location":"pipeline/pipeline/#forward","text":"Output all model predictions for each split partition. In addition, the predictions of earlier models are sent to later models of the same workflow. feed='forward'","title":"Forward"},{"location":"pipeline/pipeline/#forward-combine","text":"Output all model predictions for each split partition. In addition, the predictions of earlier models are sent to later models of all workflows. feed='forward_combine'","title":"Forward Combine"},{"location":"pipeline/pipeline/#last","text":"Output all model predictions for each split partition. In addition, the predictions of all earlier models are sent to the last model of the same workflow. feed='last'","title":"Last"},{"location":"pipeline/pipeline/#last-combine","text":"Output all model predictions for each split partition. In addition, the predictions of all earlier models are sent to the last model of all workflows. feed='last_combine'","title":"Last Combine"},{"location":"pipeline/pipeline/#examples","text":"See examples for more information","title":"Examples:"},{"location":"pipeline/split/","text":"Splits Split modules provide popular split strategies to partition data for model consumption. Rolling Window Often referred to as rolling window or walk forward optimization, this split strategy offsets each train/test split by the size of test set. split=aim.Rolling(train_size=365, test_size=180, gap_size=1, expanding_window=False) Expanding Window Often referred to as rolling window or walk forward optimization, this split strategy offsets each train/test split by the size of test set while anchoring the train set. split=aim.Rolling(train_size=365, test_size=180, gap_size=1, expanding_window=True) Train Test Split Simple train test split. split=aim.TrainTest(test_size=.3) Regime Detection Regimes with general distribution changes are detected among the market of equities. split=aim.Regimes(n_splits=5) Inspecting Start and End Points Each pipeline creates a pipeline.splits_df data frame to record the start and end dates for all train and test partitions. For example, the following are the start and end dates for a simple rolling split: Examples: See examples for more information","title":"Split"},{"location":"pipeline/split/#splits","text":"Split modules provide popular split strategies to partition data for model consumption.","title":"Splits"},{"location":"pipeline/split/#rolling-window","text":"Often referred to as rolling window or walk forward optimization, this split strategy offsets each train/test split by the size of test set. split=aim.Rolling(train_size=365, test_size=180, gap_size=1, expanding_window=False)","title":"Rolling Window"},{"location":"pipeline/split/#expanding-window","text":"Often referred to as rolling window or walk forward optimization, this split strategy offsets each train/test split by the size of test set while anchoring the train set. split=aim.Rolling(train_size=365, test_size=180, gap_size=1, expanding_window=True)","title":"Expanding Window"},{"location":"pipeline/split/#train-test-split","text":"Simple train test split. split=aim.TrainTest(test_size=.3)","title":"Train Test Split"},{"location":"pipeline/split/#regime-detection","text":"Regimes with general distribution changes are detected among the market of equities. split=aim.Regimes(n_splits=5)","title":"Regime Detection"},{"location":"pipeline/split/#inspecting-start-and-end-points","text":"Each pipeline creates a pipeline.splits_df data frame to record the start and end dates for all train and test partitions. For example, the following are the start and end dates for a simple rolling split:","title":"Inspecting Start and End Points"},{"location":"pipeline/split/#examples","text":"See examples for more information","title":"Examples:"},{"location":"pipeline/stack/","text":"Stack Architecture It's easy to stack models so that predictions from earlier models can be used as features to later models by simply defining the stack type. This makes the following possible: Meta stacking classifier Model(s) learn from all splits as well as other model(s) feature space Feed predictions from models of multiple time frames to later models All Mesh stack='all_mesh' All model predictions as inputs to all descendant split models. Example considering 3 splits: Split 0 model predictions used as inputs in all split 1 and split 2 models Split 0 and 1 model predictions used as inputs in all split 2 models Split Mesh stack='split_mesh' All model predictions used as inputs to all next split descendant models. Example considering 3 splits: Split 0 model predictions used as inputs in all split 1 models Split 1 model predictions used as inputs in all split 2 models Split Direct stack='split_direct' Last Mesh stack='last_mesh' Last Direct stack='last_direct' None Model predictions not used as inputs for other models stack=None","title":"Stack"},{"location":"pipeline/stack/#stack-architecture","text":"It's easy to stack models so that predictions from earlier models can be used as features to later models by simply defining the stack type. This makes the following possible: Meta stacking classifier Model(s) learn from all splits as well as other model(s) feature space Feed predictions from models of multiple time frames to later models","title":"Stack Architecture"},{"location":"pipeline/stack/#all-mesh","text":"stack='all_mesh' All model predictions as inputs to all descendant split models. Example considering 3 splits: Split 0 model predictions used as inputs in all split 1 and split 2 models Split 0 and 1 model predictions used as inputs in all split 2 models","title":"All Mesh"},{"location":"pipeline/stack/#split-mesh","text":"stack='split_mesh' All model predictions used as inputs to all next split descendant models. Example considering 3 splits: Split 0 model predictions used as inputs in all split 1 models Split 1 model predictions used as inputs in all split 2 models","title":"Split Mesh"},{"location":"pipeline/stack/#split-direct","text":"stack='split_direct'","title":"Split Direct"},{"location":"pipeline/stack/#last-mesh","text":"stack='last_mesh'","title":"Last Mesh"},{"location":"pipeline/stack/#last-direct","text":"stack='last_direct'","title":"Last Direct"},{"location":"pipeline/stack/#none","text":"Model predictions not used as inputs for other models stack=None","title":"None"},{"location":"tutorials/basic/","text":"This basic tutorial describes a simple use case to showcase how easy it is to use FML: FAANG equities (Facebook, Amazon, Apple, Netflix and Google) Predict next day return 3 Walk forward train/test splits Features based on lagged returns All model predictions used as inputs to descendant models Last model (3rd split) used as a stacking classifier Boiler plate It is recommended to start with boilerplate code and simply add the appropriate modules to build your workflow: if __name__ == \"__main__\": tickers = [] pipeline = Standard( stack=\"\", split=None, workflows=[{ \"targets\": [], \"features\": [], \"transforms\": [], \"reduce\": [], \"models\": [], \"stack\": {} }], algos=[], ) Pipeline Use case updated boilerplate code: if __name__ == \"__main__\": tickers = [\"FB\", \"AMZN\", \"AAPL\", \"NFLX\", \"GOOG\"] pipeline = Standard( stack=\"all_mesh\", split=Rolling(n_splits=3, gap_size=1, expanding_window=False), workflows=[{ \"targets\": [ Direction(), ], \"features\": [ LaggedReturns(), ], \"transforms\": [], \"reduce\": [], \"models\": [ PycaretBlend(), ], \"stack\": { \"target\": Direction(), \"model\": PycaretBlend(classification_models=['dt']), } }], algos=[], ) Notes: Tickers: List of equities to obtain daily price data Pipeline: Standard pipeline (only one supported currently) Stack: Enables model predictions to be used as inputs to descendant models. Split: 3 Walk forward train/test splits Workflow Target: Next day label (1 positive, 0 negative) Workflow Features: Lagged returns as feature inputs Workflow Models: Use pycaret AutoML (defaults to Random Forest) Workflow Stack: Last split uses a decision tree stacking classifier No algorithmic signals Fit and Predict FML handles splitting the data, labeling each observation, generating the features for each of the 3 models and the plumbing of stacking the models: # Fit prices = YahooCSV(tickers=tickers, frequency=['daily'], start_date='2014-01-01', end_date='2020-01-01', look_back=200) pipeline = pipeline.fit(prices) # Predict prices = YahooCSV(tickers=tickers, frequency=['daily'], start_date='2020-01-02', look_back=200) predictions = pipeline.transform(prices) Report Under construction Backtest Under construction","title":"Basic Use Case"},{"location":"tutorials/basic/#boiler-plate","text":"It is recommended to start with boilerplate code and simply add the appropriate modules to build your workflow: if __name__ == \"__main__\": tickers = [] pipeline = Standard( stack=\"\", split=None, workflows=[{ \"targets\": [], \"features\": [], \"transforms\": [], \"reduce\": [], \"models\": [], \"stack\": {} }], algos=[], )","title":"Boiler plate"},{"location":"tutorials/basic/#pipeline","text":"Use case updated boilerplate code: if __name__ == \"__main__\": tickers = [\"FB\", \"AMZN\", \"AAPL\", \"NFLX\", \"GOOG\"] pipeline = Standard( stack=\"all_mesh\", split=Rolling(n_splits=3, gap_size=1, expanding_window=False), workflows=[{ \"targets\": [ Direction(), ], \"features\": [ LaggedReturns(), ], \"transforms\": [], \"reduce\": [], \"models\": [ PycaretBlend(), ], \"stack\": { \"target\": Direction(), \"model\": PycaretBlend(classification_models=['dt']), } }], algos=[], ) Notes: Tickers: List of equities to obtain daily price data Pipeline: Standard pipeline (only one supported currently) Stack: Enables model predictions to be used as inputs to descendant models. Split: 3 Walk forward train/test splits Workflow Target: Next day label (1 positive, 0 negative) Workflow Features: Lagged returns as feature inputs Workflow Models: Use pycaret AutoML (defaults to Random Forest) Workflow Stack: Last split uses a decision tree stacking classifier No algorithmic signals","title":"Pipeline"},{"location":"tutorials/basic/#fit-and-predict","text":"FML handles splitting the data, labeling each observation, generating the features for each of the 3 models and the plumbing of stacking the models: # Fit prices = YahooCSV(tickers=tickers, frequency=['daily'], start_date='2014-01-01', end_date='2020-01-01', look_back=200) pipeline = pipeline.fit(prices) # Predict prices = YahooCSV(tickers=tickers, frequency=['daily'], start_date='2020-01-02', look_back=200) predictions = pipeline.transform(prices)","title":"Fit and Predict"},{"location":"tutorials/basic/#report","text":"Under construction","title":"Report"},{"location":"tutorials/basic/#backtest","text":"Under construction","title":"Backtest"},{"location":"tutorials/enhanced/","text":"Use Case FAANG equities (Facebook, Amazon, Apple, Netflix and Google) Predict trend direction 3 walk forward train/test splits All model predictions used as features for descendant models Last split model used as stacking classifier The first four splits will have multiple targets: Trend direction 5 day direction 1 day direction Features generated for each split: Lagged Returns QLib Catch22 Alphas101 All features pre-processed with the following transforms: Fractionally difference features Dimension reduction for each model: BoostBoruta feature selection LeaveOneFeatureOut feature selection Each split modeled with the following: Random Forest Blended SVM and Logistic Regression There is quite a bit of work that needs to be stitched together in this workflow. The amount of tasks/models required is the product of: Workflows: 1 workflow (Another tutorial will show multiple workflows) Frequency of data: Daily (Another tutorial will show multiple frequencies) Number of dimension reductions: 2 (BoostBoruta and LOFO) Number of models: 2 (RF and SVM) Targets: 3 (Trend, 1 and 5 day direction) Number of splits: 3 splits (last split used for stacking) Number of models/tasks required: 1 x 1 x 2 x 2 x 3 x 2 + 1 = 25 Internally, FML maintains the state of each task and handles the stacking of predictions as inputs into descendant models: Note the last split model receives all previous model predictions/scores as inputs along with transformed features reduced by BoostBoruta. Pipeline Using base boilerplate code with the appropriate modules: if __name__ == \"__main__\": tickers = [\"FB\", \"AMZN\", \"AAPL\", \"NFLX\", \"GOOG\"] pipeline = Standard( stack=\"all_mesh\", split=Rolling(n_splits=3, gap_size=1, expanding_window=False), workflows=[{ \"targets\": [ Direction(), ], \"features\": [ LaggedReturns(), ], \"transforms\": [], \"reduce\": [], \"models\": [ PycaretBlend(), ], \"stack\": { \"target\": Direction(), \"model\": PycaretBlend(), } }], algos=[], ) Notes: Tickers: List of equities to obtain daily price data Pipeline: Standard pipeline (only one supported currently) Stack: Enables model predictions to be used as inputs to descendant models. Split: 3 Walk forward train/test splits Workflow Target: Next day label (1 postive, 0 negative) Workflow Features: Lagged returns as feature inputs Workflow Models: Use pycaret AutoML (defaults to Random Forest) Workflow Stack: Last split used a meta stacking classifier No algorithmic signals Fit and Predict FML handles splitting the data, labeling each observation, generating the features for each of the 3 models and the plumbing of stacking the models: # Fit prices = YahooCSV(tickers=tickers, frequency=['daily'], start_date='2014-01-01', end_date='2020-01-01', look_back=200) pipeline = pipeline.fit(prices) # Predict prices = YahooCSV(tickers=tickers, frequency=['daily'], start_date='2020-01-02', look_back=200) predictions = pipeline.transform(prices) Report Under construction Backtest Under construction","title":"Enhanced Use Case"},{"location":"tutorials/enhanced/#use-case","text":"FAANG equities (Facebook, Amazon, Apple, Netflix and Google) Predict trend direction 3 walk forward train/test splits All model predictions used as features for descendant models Last split model used as stacking classifier The first four splits will have multiple targets: Trend direction 5 day direction 1 day direction Features generated for each split: Lagged Returns QLib Catch22 Alphas101 All features pre-processed with the following transforms: Fractionally difference features Dimension reduction for each model: BoostBoruta feature selection LeaveOneFeatureOut feature selection Each split modeled with the following: Random Forest Blended SVM and Logistic Regression There is quite a bit of work that needs to be stitched together in this workflow. The amount of tasks/models required is the product of: Workflows: 1 workflow (Another tutorial will show multiple workflows) Frequency of data: Daily (Another tutorial will show multiple frequencies) Number of dimension reductions: 2 (BoostBoruta and LOFO) Number of models: 2 (RF and SVM) Targets: 3 (Trend, 1 and 5 day direction) Number of splits: 3 splits (last split used for stacking) Number of models/tasks required: 1 x 1 x 2 x 2 x 3 x 2 + 1 = 25 Internally, FML maintains the state of each task and handles the stacking of predictions as inputs into descendant models: Note the last split model receives all previous model predictions/scores as inputs along with transformed features reduced by BoostBoruta.","title":"Use Case"},{"location":"tutorials/enhanced/#pipeline","text":"Using base boilerplate code with the appropriate modules: if __name__ == \"__main__\": tickers = [\"FB\", \"AMZN\", \"AAPL\", \"NFLX\", \"GOOG\"] pipeline = Standard( stack=\"all_mesh\", split=Rolling(n_splits=3, gap_size=1, expanding_window=False), workflows=[{ \"targets\": [ Direction(), ], \"features\": [ LaggedReturns(), ], \"transforms\": [], \"reduce\": [], \"models\": [ PycaretBlend(), ], \"stack\": { \"target\": Direction(), \"model\": PycaretBlend(), } }], algos=[], ) Notes: Tickers: List of equities to obtain daily price data Pipeline: Standard pipeline (only one supported currently) Stack: Enables model predictions to be used as inputs to descendant models. Split: 3 Walk forward train/test splits Workflow Target: Next day label (1 postive, 0 negative) Workflow Features: Lagged returns as feature inputs Workflow Models: Use pycaret AutoML (defaults to Random Forest) Workflow Stack: Last split used a meta stacking classifier No algorithmic signals","title":"Pipeline"},{"location":"tutorials/enhanced/#fit-and-predict","text":"FML handles splitting the data, labeling each observation, generating the features for each of the 3 models and the plumbing of stacking the models: # Fit prices = YahooCSV(tickers=tickers, frequency=['daily'], start_date='2014-01-01', end_date='2020-01-01', look_back=200) pipeline = pipeline.fit(prices) # Predict prices = YahooCSV(tickers=tickers, frequency=['daily'], start_date='2020-01-02', look_back=200) predictions = pipeline.transform(prices)","title":"Fit and Predict"},{"location":"tutorials/enhanced/#report","text":"Under construction","title":"Report"},{"location":"tutorials/enhanced/#backtest","text":"Under construction","title":"Backtest"}]}