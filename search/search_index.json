{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Financial Machine Learning framework is a modular library for automating financial machine learning workflows, integration of algorithmic strategies, and a platform for sharing reusable code. Designed with re-usable components, FML abstracts away the complexity of creating algorithmic and machine learning workflows with minimal lines of python code. Modular and reusable components such as feature engineering, transformations, dimension reduction, models, algorigmithic strategies, etc can easily be leveraged. FML also enables powerful stacking ensemble techniques across multiple data splits. Overview Quantitative trading analysts often reuse code in numerous strategies and also frequently build similar workflows. FML seeks to leverage existing and reusable code in a modular and structured fashion. The code can easily be leveraged using a standard sci-kit interface and continually improved. In addition, quant workflows generally consist of repeating processes in strategy development. FML implements flexible pipelines to rapidly build flexible and complex workflows. Professional and proprietary frameworks exist to streamline the effort, but few are open, python based and provide a flexible and structured framework to streamline the strategy lifecycle. FML can be used to leverage pre-built modules in your own workflow. This enables the reuse of code in the popular sci-kit fit/transform interface. In addition, pipelines can be configured to rapidly create customized workflows. Pipelines are easily configured by: Determine the data splits to model. Split methodologies such as rolling window (both anchored and expanding), regime detection or simple train/test split are used to define what data partitions are fed into each model. Test data (out of sample) is also defined to measure the model(s) efficacy. Choose stack architecture. The predictions of earlier models can easily be used as inputs into later models. This stacking technique enables an ensemble approach to model predictions. FML handles the plumbing necessary for many models across multiple data splits. Define one or many workflow(s). Simply select from the available modules for targets, features, transforms, feature reduction and models. Project Goals Provide a flexible, automated and easy-to-use implementation of machine learning workflows for systematic trading. The main tenants of a workflow consists of features, transformations, reduction, models and targets applied against a data split definition for multiple data frequencies. Models can be stacked across data splits in many ways, and multiple workflows can be utilized to define simple to complex configurations. Provide a comprehensive library of algorithmic and machine learning modules. All modules are sci-kit compatible with familiar fit/transform methods enabling consistent and uniform interfaces. Combine machine learning with quantitative trading strategies in an easy to use framework.","title":"Home"},{"location":"#overview","text":"Quantitative trading analysts often reuse code in numerous strategies and also frequently build similar workflows. FML seeks to leverage existing and reusable code in a modular and structured fashion. The code can easily be leveraged using a standard sci-kit interface and continually improved. In addition, quant workflows generally consist of repeating processes in strategy development. FML implements flexible pipelines to rapidly build flexible and complex workflows. Professional and proprietary frameworks exist to streamline the effort, but few are open, python based and provide a flexible and structured framework to streamline the strategy lifecycle. FML can be used to leverage pre-built modules in your own workflow. This enables the reuse of code in the popular sci-kit fit/transform interface. In addition, pipelines can be configured to rapidly create customized workflows. Pipelines are easily configured by: Determine the data splits to model. Split methodologies such as rolling window (both anchored and expanding), regime detection or simple train/test split are used to define what data partitions are fed into each model. Test data (out of sample) is also defined to measure the model(s) efficacy. Choose stack architecture. The predictions of earlier models can easily be used as inputs into later models. This stacking technique enables an ensemble approach to model predictions. FML handles the plumbing necessary for many models across multiple data splits. Define one or many workflow(s). Simply select from the available modules for targets, features, transforms, feature reduction and models.","title":"Overview"},{"location":"#project-goals","text":"Provide a flexible, automated and easy-to-use implementation of machine learning workflows for systematic trading. The main tenants of a workflow consists of features, transformations, reduction, models and targets applied against a data split definition for multiple data frequencies. Models can be stacked across data splits in many ways, and multiple workflows can be utilized to define simple to complex configurations. Provide a comprehensive library of algorithmic and machine learning modules. All modules are sci-kit compatible with familiar fit/transform methods enabling consistent and uniform interfaces. Combine machine learning with quantitative trading strategies in an easy to use framework.","title":"Project Goals"},{"location":"data/","text":"Importing Price Data AIM supports any frequency data (only a handful have been tested so far). In addition, different time frames can be used within the same pipeline. For example, feeding the predictions of a weekly model to a daily model (see feed architecture ). Data Preparation A folder named prices must exist within the current working directory containing a frequency sub-folder with ticker CSV files in the following sample hiearchy: prices/ --weekly/ --SPY.csv --GLD.csv --daily/ --SPY.csv --GLD.csv --minute15/ --SPY.csv --GLD.csv Optionally, a markets sub-folder containing groups of tickers: prices/ --markets/ --etf_sectors.csv Prices To load price data for a market of equities inside a pipeline, provide a list of tickers and frequency(s) for which the raw prices exist in the prices sub-folder. prices=aim.Prices(tickers=tickers, frequency=['daily'], start_date='2017-01-01', end_date='2020-12-31', look_back=200), tickers: List of tickers eg: ['SPY', 'GLD'] frequency: Name of prices/{frequency} sub-folder start_date: Start date of prices to import end_date: End data of prices to import look_back: Number of periods to include for feature calculation. Specifically, data prior to start date will be imported as well as for each split data partition. Returns a list of market prices for each frequency included See examples for more information","title":"Data"},{"location":"data/#importing-price-data","text":"AIM supports any frequency data (only a handful have been tested so far). In addition, different time frames can be used within the same pipeline. For example, feeding the predictions of a weekly model to a daily model (see feed architecture ).","title":"Importing Price Data"},{"location":"data/#data-preparation","text":"A folder named prices must exist within the current working directory containing a frequency sub-folder with ticker CSV files in the following sample hiearchy: prices/ --weekly/ --SPY.csv --GLD.csv --daily/ --SPY.csv --GLD.csv --minute15/ --SPY.csv --GLD.csv Optionally, a markets sub-folder containing groups of tickers: prices/ --markets/ --etf_sectors.csv","title":"Data Preparation"},{"location":"data/#prices","text":"To load price data for a market of equities inside a pipeline, provide a list of tickers and frequency(s) for which the raw prices exist in the prices sub-folder. prices=aim.Prices(tickers=tickers, frequency=['daily'], start_date='2017-01-01', end_date='2020-12-31', look_back=200), tickers: List of tickers eg: ['SPY', 'GLD'] frequency: Name of prices/{frequency} sub-folder start_date: Start date of prices to import end_date: End data of prices to import look_back: Number of periods to include for feature calculation. Specifically, data prior to start date will be imported as well as for each split data partition. Returns a list of market prices for each frequency included See examples for more information","title":"Prices"},{"location":"install/","text":"The best choice to install FML is to clone the repo and pip install -e from source. This enables active development from local source code while still using the package as if it were installed: git clone https://github.com/jmrichardson/fml.git cd fml pip install -e . Note: FML is a private repo, therefore you will need to clone using your authentication token","title":"Install"},{"location":"issues/","text":"xgboost does not support -1, 1 binary classification. Not sure if it is wise to force -1, 1 to 0, 1 Pycaret multiclass (-1, 0, 1) scoring issue. Not all models have this issue but confirmed lightgbm and nb do (others likely). It seems to happen when a split doesn't have enough data (rows or columns). This is apparently fixed in pycaret 3.0: ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].","title":"Issues"},{"location":"todo/","text":"Under Construction","title":"Pycaret Blend"},{"location":"todo_old/","text":"Add reporting using datapane. Each module should also be able to report Update backtrader module. Currently only has limited functionality Update documentation Debug all workflows and modules Upgrade to pycaret when available","title":"Todo old"},{"location":"modules/targets/direction/","text":"Label observations with future return sign (1 positive, -1 negative) Parameters: ohlcv='close': Price column of interest forward=1: Number of future periods to look ahead Example: \"targets\": [ Direction(forward=5), ],","title":"Direction"},{"location":"modules/targets/return/","text":"Label observations with future return Parameters: ohlcv='close': Price column of interest forward=1: Number of future periods to look ahead Example: \"targets\": [ Return(forward=5), ],","title":"Return"},{"location":"modules/targets/tail_sets/","text":"Tail set labels are a classification labeling technique introduced in the following paper: Nonlinear support vector machines can systematically identify stocks with high and low future returns. Algorithmic Finance, 2(1), pp.45-58. A tail set is defined to be a group of stocks whose volatility-adjusted return is in the highest or lowest quantile, for example the highest or lowest 5%. A classification model is then fit using these labels to determine which stocks to buy and sell in a long / short portfolio. Parameters: Todo Example: \"targets\": [ TailSets(), ],","title":"Tail Sets"},{"location":"modules/targets/trend_scanning/","text":"Regression and classification labeling technique that allows detection of overall trend direction. Parameters: Todo Example: \"targets\": [ TrendScanning(), ],","title":"Trend Scanning"},{"location":"modules/targets/triple_barrier/","text":"Labels observations according to the first barrier touched of out three barriers Parameters: Todo Example: \"targets\": [ TripleBarrier(), ],","title":"Triple Barrier"},{"location":"pipeline/algos/","text":"Traditional algorithmic strategies can be included in a pipeline by simply adding the appropriate module(s). FML will simply concatenate the strategy signal(s) with workflow signals(s) if included. For example, the following could be used to output only algorithmic signals (no ML workflow): pipeline = Standard( algos=[ PatternDifferentials(differential=1), ContrarianAugmentedBollingerBands(), ], )","title":"Algos"},{"location":"pipeline/split/","text":"Splits Split modules provide popular split strategies to partition data for model consumption. Rolling Window Often referred to as rolling window or walk forward optimization, this split strategy offsets each train/test split by the size of test set. split=Rolling(n_splits=3, gap_size=1, expanding_window=False) split=aim.Rolling(train_size=365, test_size=180, gap_size=1, expanding_window=False) Expanding Window Often referred to as rolling window or walk forward optimization, this split strategy offsets each train/test split by the size of test set while anchoring the train set. split=Rolling(n_splits=3, gap_size=1, expanding_window=True) split=aim.Rolling(train_size=365, test_size=180, gap_size=1, expanding_window=True) Train Test Split Simple train test split. split=aim.TrainTest(test_size=.3) Regime Detection Regimes with general distribution changes are detected among the market of equities. split=aim.Regimes(n_splits=5)","title":"Split"},{"location":"pipeline/split/#splits","text":"Split modules provide popular split strategies to partition data for model consumption.","title":"Splits"},{"location":"pipeline/split/#rolling-window","text":"Often referred to as rolling window or walk forward optimization, this split strategy offsets each train/test split by the size of test set. split=Rolling(n_splits=3, gap_size=1, expanding_window=False) split=aim.Rolling(train_size=365, test_size=180, gap_size=1, expanding_window=False)","title":"Rolling Window"},{"location":"pipeline/split/#expanding-window","text":"Often referred to as rolling window or walk forward optimization, this split strategy offsets each train/test split by the size of test set while anchoring the train set. split=Rolling(n_splits=3, gap_size=1, expanding_window=True) split=aim.Rolling(train_size=365, test_size=180, gap_size=1, expanding_window=True)","title":"Expanding Window"},{"location":"pipeline/split/#train-test-split","text":"Simple train test split. split=aim.TrainTest(test_size=.3)","title":"Train Test Split"},{"location":"pipeline/split/#regime-detection","text":"Regimes with general distribution changes are detected among the market of equities. split=aim.Regimes(n_splits=5)","title":"Regime Detection"},{"location":"pipeline/stack/","text":"Stacking is an ensemble learning technique which combines multiple models via a meta-classifier. FML handles the plumbing required to feed model predictions forward based on the stack architecture defined. Stack Architecture It's easy to stack models so that predictions from earlier models can be used as features in later models by simply defining the stack type. This makes the following possible: Meta stacking classifier Model(s) learn from all splits as well as other model(s) feature space Feed predictions from models of multiple time frames to later models Note \"all_mesh\" is currently supported. Others are not fully implemented All Mesh stack='all_mesh' All model predictions used as inputs in all future models. Example considering 3 splits: All Split 1 models receive predictions from all split 0 models Split 2 classifier model receive predictions from all split 0 and split 1 models Split Mesh stack='split_mesh' All models forward their predictions all next split future models. Example considering 3 splits: All Split 1 models receive predictions from all split 0 models Split 2 classifier model receive predictions from split 1 models Split Direct stack='split_direct' All models forward their predictions directly to their next split future model. Example considering 3 splits: Split 0 models forward predictions directly to split 1 future model Split 2 classifier model receives predictions from all split 1 models Last Mesh stack='last_mesh' Last Direct stack='last_direct' None Model predictions not used as inputs for other models stack=None","title":"Stack"},{"location":"pipeline/stack/#stack-architecture","text":"It's easy to stack models so that predictions from earlier models can be used as features in later models by simply defining the stack type. This makes the following possible: Meta stacking classifier Model(s) learn from all splits as well as other model(s) feature space Feed predictions from models of multiple time frames to later models Note \"all_mesh\" is currently supported. Others are not fully implemented","title":"Stack Architecture"},{"location":"pipeline/stack/#all-mesh","text":"stack='all_mesh' All model predictions used as inputs in all future models. Example considering 3 splits: All Split 1 models receive predictions from all split 0 models Split 2 classifier model receive predictions from all split 0 and split 1 models","title":"All Mesh"},{"location":"pipeline/stack/#split-mesh","text":"stack='split_mesh' All models forward their predictions all next split future models. Example considering 3 splits: All Split 1 models receive predictions from all split 0 models Split 2 classifier model receive predictions from split 1 models","title":"Split Mesh"},{"location":"pipeline/stack/#split-direct","text":"stack='split_direct' All models forward their predictions directly to their next split future model. Example considering 3 splits: Split 0 models forward predictions directly to split 1 future model Split 2 classifier model receives predictions from all split 1 models","title":"Split Direct"},{"location":"pipeline/stack/#last-mesh","text":"stack='last_mesh'","title":"Last Mesh"},{"location":"pipeline/stack/#last-direct","text":"stack='last_direct'","title":"Last Direct"},{"location":"pipeline/stack/#none","text":"Model predictions not used as inputs for other models stack=None","title":"None"},{"location":"pipeline/workflows/","text":"Workflows represent the process of labeling OHLCV data, preparing features and processing features and training models for each data split. Modules are provided to customize each step while FML handles the required work to make predictions. Notes You can have multiple workflows to provide even more flexibility such as providing different subsets of features. For example, one workflow could be macro economic based, another price based and another sentiment based. Stacking is optional; all model predictions are produced for further processing","title":"Workflows"},{"location":"pipeline/workflows/#notes","text":"You can have multiple workflows to provide even more flexibility such as providing different subsets of features. For example, one workflow could be macro economic based, another price based and another sentiment based. Stacking is optional; all model predictions are produced for further processing","title":"Notes"},{"location":"tutorials/basic/","text":"This basic tutorial describes a simple use case to showcase how easy it is to use FML: FAANG equities (Facebook, Amazon, Apple, Netflix and Google) Predict next day return 3 Walk forward train/test splits Features based on lagged returns All model predictions used as inputs to descendant models Last model (3rd split) used as a stacking classifier Boiler plate It is recommended to start with boilerplate code and simply add the appropriate modules to build your workflow: if __name__ == \"__main__\": tickers = [] pipeline = Standard( stack=\"\", split=None, workflows=[{ \"targets\": [], \"features\": [], \"transforms\": [], \"reduce\": [], \"models\": [], \"stack\": {} }], algos=[], ) Pipeline Use case updated boilerplate code: if __name__ == \"__main__\": tickers = [\"FB\", \"AMZN\", \"AAPL\", \"NFLX\", \"GOOG\"] pipeline = Standard( stack=\"all_mesh\", split=Rolling(n_splits=3, gap_size=1, expanding_window=False), workflows=[{ \"targets\": [ Direction(), ], \"features\": [ LaggedReturns(), ], \"transforms\": [], \"reduce\": [], \"models\": [ PycaretBlend(), ], \"stack\": { \"target\": Direction(), \"model\": PycaretBlend(classification_models=['dt']), } }], algos=[], ) Notes: Tickers: List of equities to obtain daily price data Pipeline: Standard pipeline (only one supported currently) Stack: Enables model predictions to be used as inputs to descendant models. Split: 3 Walk forward train/test splits Workflow Target: Next day label (1 positive, 0 negative) Workflow Features: Lagged returns as feature inputs Workflow Models: Use pycaret AutoML (defaults to Random Forest) Workflow Stack: Last split uses a decision tree stacking classifier No algorithmic signals Fit and Predict FML handles splitting the data, labeling each observation, generating the features for each of the 3 models and the plumbing of stacking the models: # Fit prices = YahooCSV(tickers=tickers, frequency=['daily'], start_date='2014-01-01', end_date='2020-01-01', look_back=200) pipeline = pipeline.fit(prices) # Predict prices = YahooCSV(tickers=tickers, frequency=['daily'], start_date='2020-01-02', look_back=200) predictions = pipeline.transform(prices) Report Under construction Backtest Under construction","title":"Basic Use Case"},{"location":"tutorials/basic/#boiler-plate","text":"It is recommended to start with boilerplate code and simply add the appropriate modules to build your workflow: if __name__ == \"__main__\": tickers = [] pipeline = Standard( stack=\"\", split=None, workflows=[{ \"targets\": [], \"features\": [], \"transforms\": [], \"reduce\": [], \"models\": [], \"stack\": {} }], algos=[], )","title":"Boiler plate"},{"location":"tutorials/basic/#pipeline","text":"Use case updated boilerplate code: if __name__ == \"__main__\": tickers = [\"FB\", \"AMZN\", \"AAPL\", \"NFLX\", \"GOOG\"] pipeline = Standard( stack=\"all_mesh\", split=Rolling(n_splits=3, gap_size=1, expanding_window=False), workflows=[{ \"targets\": [ Direction(), ], \"features\": [ LaggedReturns(), ], \"transforms\": [], \"reduce\": [], \"models\": [ PycaretBlend(), ], \"stack\": { \"target\": Direction(), \"model\": PycaretBlend(classification_models=['dt']), } }], algos=[], ) Notes: Tickers: List of equities to obtain daily price data Pipeline: Standard pipeline (only one supported currently) Stack: Enables model predictions to be used as inputs to descendant models. Split: 3 Walk forward train/test splits Workflow Target: Next day label (1 positive, 0 negative) Workflow Features: Lagged returns as feature inputs Workflow Models: Use pycaret AutoML (defaults to Random Forest) Workflow Stack: Last split uses a decision tree stacking classifier No algorithmic signals","title":"Pipeline"},{"location":"tutorials/basic/#fit-and-predict","text":"FML handles splitting the data, labeling each observation, generating the features for each of the 3 models and the plumbing of stacking the models: # Fit prices = YahooCSV(tickers=tickers, frequency=['daily'], start_date='2014-01-01', end_date='2020-01-01', look_back=200) pipeline = pipeline.fit(prices) # Predict prices = YahooCSV(tickers=tickers, frequency=['daily'], start_date='2020-01-02', look_back=200) predictions = pipeline.transform(prices)","title":"Fit and Predict"},{"location":"tutorials/basic/#report","text":"Under construction","title":"Report"},{"location":"tutorials/basic/#backtest","text":"Under construction","title":"Backtest"},{"location":"tutorials/enhanced/","text":"Use Case FAANG equities (Facebook, Amazon, Apple, Netflix and Google) Forecast 5 day direction All model predictions used as features for future models 3 walk forward train/test splits The first 2 splits will have multiple targets: Trend direction 5 day direction 1 day direction Last split model used as stacking classifier Features generated for each split: Lagged Returns QLib Catch22 Alphas101 All features pre-processed with the following transforms: Fractionally difference features Dimension reduction for each model: BoostBoruta feature selection LeaveOneFeatureOut feature selection Each split modeled with the following: Random Forest Support Vector Machine There is quite a bit of work that needs to be stitched together in this workflow. The amount of tasks/models required is the product of: Workflows: 1 workflow Frequency of data: Daily Number of dimension reductions: 2 (BoostBoruta and LOFO) Number of models: 2 (RF and SVM) Targets: 3 (Trend, 1 and 5 day direction) Number of splits: 3 splits (last split used for stacking) Number of models/tasks required: 1 x 1 x 2 x 2 x 3 x 2 + 1 = 25 Internally, FML maintains the state of each task and handles the stacking of predictions as inputs into future models: Note the last split model receives all previous model predictions and scores as inputs along with transformed features reduced by BoostBoruta. Pipeline Using boilerplate code with the appropriate modules: if __name__ == \"__main__\": tickers = [\"FB\", \"AMZN\", \"AAPL\", \"NFLX\", \"GOOG\"] pipeline = Standard( stack=\"all_mesh\", split=Rolling(n_splits=3, gap_size=1, expanding_window=False), workflows=[{ \"targets\": [ Direction(forward=1), Direction(forward=5), TrendScanning() ], \"features\": [ LaggedReturns(), QLib(), Catch22(), Alphas101(), ], \"transforms\": [ FracDiff(), ], \"reduce\": [ LeaveOneFeatureOut(quantile=.25), BoostBoruta(), ], \"models\": [ PycaretBlend(classification_models=[\"rf\"]), PycaretBlend(classification_models=[\"svm\"]), ], \"stack\": { \"reduce\": BoostBoruta(), \"target\": Direction(forward=5), \"model\": PycaretBlend(classification_models=[\"rf\"]), } }], algos=[], )","title":"Enhanced Use Case"},{"location":"tutorials/enhanced/#use-case","text":"FAANG equities (Facebook, Amazon, Apple, Netflix and Google) Forecast 5 day direction All model predictions used as features for future models 3 walk forward train/test splits The first 2 splits will have multiple targets: Trend direction 5 day direction 1 day direction Last split model used as stacking classifier Features generated for each split: Lagged Returns QLib Catch22 Alphas101 All features pre-processed with the following transforms: Fractionally difference features Dimension reduction for each model: BoostBoruta feature selection LeaveOneFeatureOut feature selection Each split modeled with the following: Random Forest Support Vector Machine There is quite a bit of work that needs to be stitched together in this workflow. The amount of tasks/models required is the product of: Workflows: 1 workflow Frequency of data: Daily Number of dimension reductions: 2 (BoostBoruta and LOFO) Number of models: 2 (RF and SVM) Targets: 3 (Trend, 1 and 5 day direction) Number of splits: 3 splits (last split used for stacking) Number of models/tasks required: 1 x 1 x 2 x 2 x 3 x 2 + 1 = 25 Internally, FML maintains the state of each task and handles the stacking of predictions as inputs into future models: Note the last split model receives all previous model predictions and scores as inputs along with transformed features reduced by BoostBoruta.","title":"Use Case"},{"location":"tutorials/enhanced/#pipeline","text":"Using boilerplate code with the appropriate modules: if __name__ == \"__main__\": tickers = [\"FB\", \"AMZN\", \"AAPL\", \"NFLX\", \"GOOG\"] pipeline = Standard( stack=\"all_mesh\", split=Rolling(n_splits=3, gap_size=1, expanding_window=False), workflows=[{ \"targets\": [ Direction(forward=1), Direction(forward=5), TrendScanning() ], \"features\": [ LaggedReturns(), QLib(), Catch22(), Alphas101(), ], \"transforms\": [ FracDiff(), ], \"reduce\": [ LeaveOneFeatureOut(quantile=.25), BoostBoruta(), ], \"models\": [ PycaretBlend(classification_models=[\"rf\"]), PycaretBlend(classification_models=[\"svm\"]), ], \"stack\": { \"reduce\": BoostBoruta(), \"target\": Direction(forward=5), \"model\": PycaretBlend(classification_models=[\"rf\"]), } }], algos=[], )","title":"Pipeline"}]}