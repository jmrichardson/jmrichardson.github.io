{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Financial Machine Learning framework is a modular library for automating machine learning workflows and integration of algorithmic strategies for systematic trading. Designed with re-usable components, FML abstracts away the complexity of creating machine learning pipelines with minimal lines of python code. Modular and reusable components such as feature engineering, transformations, models, strategies, etc can easily be leveraged from the community. Project Goals Provide a comprehensive library of machine learning workflow modules. Leveraging contributed component modules, we take advantage of the power of the community. All modules are sci-kit compatible with familiar fit/transform methods enabling consistent and uniform interfaces. Provide a flexible, automated and easy-to-use implementation of machine learning workflows for systematic trading. The main tenants of a workflow consists of features, transformations, models and targets applied against a data split definition for multiple data frequencies. Models can be stacked across data splits in many ways, and multiple workflows can be utilized to define simple to complex configurations. Combine machine learning with quantitative trading strategies in an easy to use framework. Overview A custom pipeline is used to create process workflows: Determine the data splits to model. Split methodologies such as rolling window (both anchored and expanding), regime detection or simple train/test split are used to define what data partitions are fed into each model. Test data (out of sample) is also defined to measure the model(s) efficacy. Choose feed architecture. The predictions of earlier models can easily be used as inputs into later models. This powerful technique enables an ensemble approach to model predictions. Architectures consist of feed forward, forward combine, last, and last combine. Similar to traditional ensemble methods for a single data split, FML handles this process across many models for multiple data splits. All model fits for all data splits are stored and used appropriately for prediction. Define one or many workflow(s). Simply select from the available modules for targets, features, transforms and models. FML handles all of the plumbing of the architecture for each step of the pipeline. Smart caching is utilized to avoid time consuming repetitive tasks. In addition, FML consists of reusable modules that can each be leveraged as needed for repeatable tasks.","title":"Home"},{"location":"#project-goals","text":"Provide a comprehensive library of machine learning workflow modules. Leveraging contributed component modules, we take advantage of the power of the community. All modules are sci-kit compatible with familiar fit/transform methods enabling consistent and uniform interfaces. Provide a flexible, automated and easy-to-use implementation of machine learning workflows for systematic trading. The main tenants of a workflow consists of features, transformations, models and targets applied against a data split definition for multiple data frequencies. Models can be stacked across data splits in many ways, and multiple workflows can be utilized to define simple to complex configurations. Combine machine learning with quantitative trading strategies in an easy to use framework.","title":"Project Goals"},{"location":"#overview","text":"A custom pipeline is used to create process workflows: Determine the data splits to model. Split methodologies such as rolling window (both anchored and expanding), regime detection or simple train/test split are used to define what data partitions are fed into each model. Test data (out of sample) is also defined to measure the model(s) efficacy. Choose feed architecture. The predictions of earlier models can easily be used as inputs into later models. This powerful technique enables an ensemble approach to model predictions. Architectures consist of feed forward, forward combine, last, and last combine. Similar to traditional ensemble methods for a single data split, FML handles this process across many models for multiple data splits. All model fits for all data splits are stored and used appropriately for prediction. Define one or many workflow(s). Simply select from the available modules for targets, features, transforms and models. FML handles all of the plumbing of the architecture for each step of the pipeline. Smart caching is utilized to avoid time consuming repetitive tasks. In addition, FML consists of reusable modules that can each be leveraged as needed for repeatable tasks.","title":"Overview"},{"location":"Install/","text":"Install Use pip to install latest version: pip install git+https://{token}@github.com/jmrichardson/aim.git","title":"Install"},{"location":"Install/#install","text":"Use pip to install latest version: pip install git+https://{token}@github.com/jmrichardson/aim.git","title":"Install"},{"location":"components/","text":"Available Component Modules Data Prices Splits Rolling window (walk forward) Rolling expanding window (walk forward) Dynamic Regime Detection Train Test Split Feed Architecture None Forward Forward Combine Last Last Combine Pre-process To be added Feature Engineering TuneTA Formulaic alphas 101 Catch 22 Qlib Lagged returns Date time features Transformations Remove constant features Fractional difference stationary features Feature selection (BoostBoruta) Models Pycaret Targets Direction Forward Return Triple Barrier Label Trend Scanning Tail Sets Events Cusum Signals Moving Average Crossover","title":"Components"},{"location":"components/#available-component-modules","text":"","title":"Available Component Modules"},{"location":"components/#data","text":"Prices","title":"Data"},{"location":"components/#splits","text":"Rolling window (walk forward) Rolling expanding window (walk forward) Dynamic Regime Detection Train Test Split","title":"Splits"},{"location":"components/#feed-architecture","text":"None Forward Forward Combine Last Last Combine","title":"Feed Architecture"},{"location":"components/#pre-process","text":"To be added","title":"Pre-process"},{"location":"components/#feature-engineering","text":"TuneTA Formulaic alphas 101 Catch 22 Qlib Lagged returns Date time features","title":"Feature Engineering"},{"location":"components/#transformations","text":"Remove constant features Fractional difference stationary features Feature selection (BoostBoruta)","title":"Transformations"},{"location":"components/#models","text":"Pycaret","title":"Models"},{"location":"components/#targets","text":"Direction Forward Return Triple Barrier Label Trend Scanning Tail Sets","title":"Targets"},{"location":"components/#events","text":"Cusum","title":"Events"},{"location":"components/#signals","text":"Moving Average Crossover","title":"Signals"},{"location":"data/","text":"Importing Price Data AIM supports any frequency data (only a handful have been tested so far). In addition, different time frames can be used within the same pipeline. For example, feeding the predictions of a weekly model to a daily model (see feed architecture ). Data Preparation A folder named prices must exist within the current working directory containing a frequency sub-folder with ticker CSV files in the following sample hiearchy: prices/ --weekly/ --SPY.csv --GLD.csv --daily/ --SPY.csv --GLD.csv --minute15/ --SPY.csv --GLD.csv Optionally, a markets sub-folder containing groups of tickers: prices/ --markets/ --etf_sectors.csv Prices To load price data for a market of equities inside a pipeline, provide a list of tickers and frequency(s) for which the raw prices exist in the prices sub-folder. prices=aim.Prices(tickers=tickers, frequency=['daily'], start_date='2017-01-01', end_date='2020-12-31', look_back=200), tickers: List of tickers eg: ['SPY', 'GLD'] frequency: Name of prices/{frequency} sub-folder start_date: Start date of prices to import end_date: End data of prices to import look_back: Number of periods to include for feature calculation. Specifically, data prior to start date will be imported as well as for each split data partition. Returns a list of market prices for each frequency included See examples for more information","title":"Data"},{"location":"data/#importing-price-data","text":"AIM supports any frequency data (only a handful have been tested so far). In addition, different time frames can be used within the same pipeline. For example, feeding the predictions of a weekly model to a daily model (see feed architecture ).","title":"Importing Price Data"},{"location":"data/#data-preparation","text":"A folder named prices must exist within the current working directory containing a frequency sub-folder with ticker CSV files in the following sample hiearchy: prices/ --weekly/ --SPY.csv --GLD.csv --daily/ --SPY.csv --GLD.csv --minute15/ --SPY.csv --GLD.csv Optionally, a markets sub-folder containing groups of tickers: prices/ --markets/ --etf_sectors.csv","title":"Data Preparation"},{"location":"data/#prices","text":"To load price data for a market of equities inside a pipeline, provide a list of tickers and frequency(s) for which the raw prices exist in the prices sub-folder. prices=aim.Prices(tickers=tickers, frequency=['daily'], start_date='2017-01-01', end_date='2020-12-31', look_back=200), tickers: List of tickers eg: ['SPY', 'GLD'] frequency: Name of prices/{frequency} sub-folder start_date: Start date of prices to import end_date: End data of prices to import look_back: Number of periods to include for feature calculation. Specifically, data prior to start date will be imported as well as for each split data partition. Returns a list of market prices for each frequency included See examples for more information","title":"Prices"},{"location":"feed/","text":"Feed Architecture It's easy to create independent models within a pipeline. It's also easy to stack models so that predictions from earlier models can be used as features to later models by simply defining the feed type. This makes the following possible: Ensemble/stacking approach Model(s) learn from all splits as well as other model(s) feature space Feed predictions from models of multiple time frames to later models Feature space reduction partitioned by model and integrated in successive models All model predictions regardless of split are sent to final Predictions module where simple weighting or other aggregation methodology can be leveraged None Output all model predictions for each split partition. feed=None Forward Output all model predictions for each split partition. In addition, the predictions of earlier models are sent to later models of the same workflow. feed='forward' Forward Combine Output all model predictions for each split partition. In addition, the predictions of earlier models are sent to later models of all workflows. feed='forward_combine' Last Output all model predictions for each split partition. In addition, the predictions of all earlier models are sent to the last model of the same workflow. feed='last' Last Combine Output all model predictions for each split partition. In addition, the predictions of all earlier models are sent to the last model of all workflows. feed='last_combine' Examples: See examples for more information","title":"Feed"},{"location":"feed/#feed-architecture","text":"It's easy to create independent models within a pipeline. It's also easy to stack models so that predictions from earlier models can be used as features to later models by simply defining the feed type. This makes the following possible: Ensemble/stacking approach Model(s) learn from all splits as well as other model(s) feature space Feed predictions from models of multiple time frames to later models Feature space reduction partitioned by model and integrated in successive models All model predictions regardless of split are sent to final Predictions module where simple weighting or other aggregation methodology can be leveraged","title":"Feed Architecture"},{"location":"feed/#none","text":"Output all model predictions for each split partition. feed=None","title":"None"},{"location":"feed/#forward","text":"Output all model predictions for each split partition. In addition, the predictions of earlier models are sent to later models of the same workflow. feed='forward'","title":"Forward"},{"location":"feed/#forward-combine","text":"Output all model predictions for each split partition. In addition, the predictions of earlier models are sent to later models of all workflows. feed='forward_combine'","title":"Forward Combine"},{"location":"feed/#last","text":"Output all model predictions for each split partition. In addition, the predictions of all earlier models are sent to the last model of the same workflow. feed='last'","title":"Last"},{"location":"feed/#last-combine","text":"Output all model predictions for each split partition. In addition, the predictions of all earlier models are sent to the last model of all workflows. feed='last_combine'","title":"Last Combine"},{"location":"feed/#examples","text":"See examples for more information","title":"Examples:"},{"location":"signals/","text":"Signals Quantitative trading strategies can be combined with machine learning predictions. The final predictions module can be used to aggregate external signals with the predictions from workflows into a final prediction. Note, some sophisticated signals may require a fit process. Therefore, all signals are passed the entire range of training data (no splits) for any fits required. MA Crossover Simple moving average crossover signal MaCrossover calculates both a fast and slow period moving average. Returns 1 if fast is above slow, and 0 otherwise.","title":"Signals"},{"location":"signals/#signals","text":"Quantitative trading strategies can be combined with machine learning predictions. The final predictions module can be used to aggregate external signals with the predictions from workflows into a final prediction. Note, some sophisticated signals may require a fit process. Therefore, all signals are passed the entire range of training data (no splits) for any fits required.","title":"Signals"},{"location":"signals/#ma-crossover","text":"Simple moving average crossover signal MaCrossover calculates both a fast and slow period moving average. Returns 1 if fast is above slow, and 0 otherwise.","title":"MA Crossover"},{"location":"splits/","text":"Splits Split modules provide popular split strategies to partition data for model consumption. Rolling Window Often referred to as rolling window or walk forward optimization, this split strategy offsets each train/test split by the size of test set. split=aim.Rolling(train_size=365, test_size=180, gap_size=1, expanding_window=False) Expanding Window Often referred to as rolling window or walk forward optimization, this split strategy offsets each train/test split by the size of test set while anchoring the train set. split=aim.Rolling(train_size=365, test_size=180, gap_size=1, expanding_window=True) Train Test Split Simple train test split. split=aim.TrainTest(test_size=.3) Regime Detection Regimes with general distribution changes are detected among the market of equities. split=aim.Regimes(n_splits=5) Inspecting Start and End Points Each pipeline creates a pipeline.splits_df data frame to record the start and end dates for all train and test partitions. For example, the following are the start and end dates for a simple rolling split: Examples: See examples for more information","title":"Splits"},{"location":"splits/#splits","text":"Split modules provide popular split strategies to partition data for model consumption.","title":"Splits"},{"location":"splits/#rolling-window","text":"Often referred to as rolling window or walk forward optimization, this split strategy offsets each train/test split by the size of test set. split=aim.Rolling(train_size=365, test_size=180, gap_size=1, expanding_window=False)","title":"Rolling Window"},{"location":"splits/#expanding-window","text":"Often referred to as rolling window or walk forward optimization, this split strategy offsets each train/test split by the size of test set while anchoring the train set. split=aim.Rolling(train_size=365, test_size=180, gap_size=1, expanding_window=True)","title":"Expanding Window"},{"location":"splits/#train-test-split","text":"Simple train test split. split=aim.TrainTest(test_size=.3)","title":"Train Test Split"},{"location":"splits/#regime-detection","text":"Regimes with general distribution changes are detected among the market of equities. split=aim.Regimes(n_splits=5)","title":"Regime Detection"},{"location":"splits/#inspecting-start-and-end-points","text":"Each pipeline creates a pipeline.splits_df data frame to record the start and end dates for all train and test partitions. For example, the following are the start and end dates for a simple rolling split:","title":"Inspecting Start and End Points"},{"location":"splits/#examples","text":"See examples for more information","title":"Examples:"}]}