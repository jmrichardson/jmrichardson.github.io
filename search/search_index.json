{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Use Hypernets as example github</p>"},{"location":"#financial-machine-learning-framework","title":"Financial Machine Learning Framework","text":"<p>FML is a modular framework for automating financial machine learning pipelines using modular and reusable code.  FML is currently being integrated with the NumerAI platform.  </p>"},{"location":"#features","title":"Features","text":"<ul> <li>Simple abstraction for creating complex machine learning pipelines</li> <li>Reuse existing code with scikit interface</li> <li>Rapidly test flexible pipelines with minimal code</li> <li>Modules include data preprocessing, partitioning, models, targets, feature engineering,  transforms, and selection</li> <li>Feature neutralization using test or training data</li> <li>Model ensembles with equal or modern portfolio optimization weighting</li> <li>Performance report for validation</li> <li>Easy parameter optimization using Optuna</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>Understanding Pipelines</li> <li>Creating Modules</li> </ul>"},{"location":"#why-fml","title":"Why FML?","text":"<p>It would be nice to create a model and then just simply include it in a pipeline.  It would be great to reuse the feature selection code I wrote for another project.  It would also be nice to have flexibility in creating new pipelines without rewriting the pipeline code. </p> <p>The traditional way is to maintain a larger pipeline script and inject the relevant code.  This can become difficult to manage as complexity increases.  A framework that simply calls the fit and predict of modular code while hiding the pipeline complexity provides a much more efficient test and deploy methodology.  The additional benefit is over time, you build a collection of organized and reusable code for pipeline modules such as models, targets, features, selection, etc.    FML makes it easy to create complex pipelines with low effort while making idea development significantly easier.    </p>"},{"location":"#todo","title":"Todo","text":"<ul> <li>Model stacking (model predictions used as features for downstream models)</li> <li>Numerai signal integration</li> <li>More modules</li> </ul>"},{"location":"install/","title":"Install","text":"<p>The best choice to install FML is to clone the repo and <code>pip install -e</code> from source.  This enables active development from local source code while still using the package as if it were installed:</p> <pre><code>git clone https://github.com/jmrichardson/fml.git\ncd fml\nconda create --name fml python=3.8 -y\nconda activate fml\npip install --use-deprecated=legacy-resolver -e .\npip install --force-reinstall --no-cache-dir --no-deps TA-Lib==0.4.24\n</code></pre>"},{"location":"modules/","title":"Modules","text":""},{"location":"modules/#creating-modules","title":"Creating Modules","text":"<p>Modules are the building blocks of FML pipelines.  Creating a module is simply using boiler plate code with a fit and predict or transform interface. The easiest way to start is to copy a similar module's file and remove the existing code.  There are a few things to keep in mind:</p> <p>Some modules have caching code to reduce long running time.  If your module doesn't take significant time to execute then this code can be removed.  </p> <p>Fit functions are provided both the era and label for the data partition.  Predict and transform do not include the label</p> <p>There is minimal error checking in the pipeline for data integrity.  It is advisable to use debug stop points to ensure returned data is accurate.</p>"},{"location":"pipelines/","title":"Pipelines","text":""},{"location":"pipelines/#understanding-pipelines","title":"Understanding Pipelines","text":"<p>FML defines a pipeline as a collection of workflows each consisting of individual tasks.  A task is defined by an associated user created module.</p> <p>The following types of modules can be used to define a workflow.</p> <ul> <li>Preprocess: modifies the data before it is partitioned in a workflow</li> <li>Partitions: splits the training data</li> <li>Features: appends new features to existing features in the data partition</li> <li>Transforms: transforms the data features</li> <li>Select: Feature reduction</li> <li>Target: training target</li> <li>Model: build model</li> </ul> <p>Partitions, Select, Target and Model modules determine the number of tasks per workflow.  The number of tasks is the combination of each of these modules.  For example, if your workflow has 2 partitions, 2 selects, 3 targets and 3 models will result in 36 tasks for the workflow:</p> <pre><code>2 * 2 * 3 * 3 = 36\n</code></pre> <p>In other words, a total of 36 unique models will be created.   It's important to note that features and transforms modules will be performed on each of the 36 tasks in the workflow but do not create unique tasks.  Preprocess occurs once for the entire workflow and therefore also doesn't create unique tasks.    </p> <p>Note, If you would prefer to manually control  generated features for separate models as opposed to feature selection, define multiple workflows.  The flexibility of multiple workflows allows for limitless customization of the pipeline.</p>"},{"location":"quick_start/","title":"Quick start","text":""},{"location":"quick_start/#quick-start","title":"Quick Start","text":"<p>Let's build a NumerAI tournament pipeline with the following architecture:</p> <p>Use all training data Use nomi 20 day target  LGBM with NumerAI default parameters </p> <p>Let's try to improve the pipeline:</p> <p>Partition training data using every 4th era for a total of 4 training sets.  Train each model using the average of all the provided 20 day targets. Use LGBM model with default NumerAI parameters Ensemble all model predictions with modern portfolio optimization weights</p> <p>Now we can improve the pipeline a bit more:</p> <p>Use the entire training data set as well as every 4th era Increase the number of trees to 4000 instead of 2000 Neutralize features</p>"}]}